{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Reviews and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will be working with the [Yelp dataset](http://cs-people.bu.edu/kzhao/teaching/yelp_dataset_challenge_academic_dataset.tar). You can find the format of the dataset [here](https://www.yelp.com/dataset_challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will look at Review Objects and perform some [sentiment analysis](http://sentiment.christopherpotts.net/) on the review text.\n",
    "\n",
    "You will need to preprocess the text using a stemming algorithm. The Porter stemming algorithm is a well-known one. Then, use a lexicon to assign a score to a review based on the positive/negative words you find in the text. You can find various lexicons [here](http://sentiment.christopherpotts.net/lexicons.html).\n",
    "\n",
    "After you have assigned scores to the reviews based on the text analysis, compare your scores with the stars associated with the reviews. **(20 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "3\n",
      "4s\n",
      "1\n",
      "4s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "3s\n",
      "1\n",
      "3s\n",
      "1\n",
      "3s\n",
      "4\n",
      "3s\n",
      "4\n",
      "3s\n",
      "5\n",
      "3s\n",
      "2\n",
      "4s\n",
      "4\n",
      "1s\n",
      "1\n",
      "3s\n",
      "1\n",
      "3s\n",
      "5\n",
      "5s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "4\n",
      "5s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "4\n",
      "3s\n",
      "5\n",
      "4s\n",
      "4\n",
      "3s\n",
      "5\n",
      "4s\n",
      "4\n",
      "4s\n",
      "4\n",
      "5s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "1s\n",
      "4\n",
      "3s\n",
      "4\n",
      "4s\n",
      "3\n",
      "3s\n",
      "4\n",
      "5s\n",
      "5\n",
      "4s\n",
      "3\n",
      "5s\n",
      "5\n",
      "1s\n",
      "2\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "3\n",
      "3s\n",
      "3\n",
      "5s\n",
      "3\n",
      "5s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "4\n",
      "3s\n",
      "1\n",
      "1s\n",
      "1\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "1s\n",
      "1\n",
      "3s\n",
      "5\n",
      "3s\n",
      "4\n",
      "3s\n",
      "4\n",
      "3s\n",
      "4\n",
      "1s\n",
      "4\n",
      "3s\n",
      "4\n",
      "3s\n",
      "1\n",
      "3s\n",
      "2\n",
      "3s\n",
      "4\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "5s\n",
      "4\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "5s\n",
      "5\n",
      "3s\n",
      "3\n",
      "3s\n",
      "3\n",
      "1s\n",
      "3\n",
      "3s\n",
      "4\n",
      "3s\n",
      "2\n",
      "4s\n",
      "4\n",
      "3s\n",
      "4\n",
      "3s\n",
      "3\n",
      "3s\n",
      "2\n",
      "3s\n",
      "3\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "1s\n",
      "1\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "2\n",
      "4s\n",
      "3\n",
      "3s\n",
      "1\n",
      "3s\n",
      "2\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "3\n",
      "5s\n",
      "4\n",
      "4s\n",
      "3\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "5\n",
      "4s\n",
      "4\n",
      "4s\n",
      "4\n",
      "5s\n",
      "4\n",
      "3s\n",
      "5\n",
      "3s\n",
      "3\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "1s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "4\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "5s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "5\n",
      "1s\n",
      "3\n",
      "3s\n",
      "4\n",
      "4s\n",
      "3\n",
      "5s\n",
      "5\n",
      "4s\n",
      "4\n",
      "4s\n",
      "5\n",
      "3s\n",
      "2\n",
      "3s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "5s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "2\n",
      "3s\n",
      "3\n",
      "3s\n",
      "3\n",
      "4s\n",
      "4\n",
      "3s\n",
      "4\n",
      "3s\n",
      "4\n",
      "5s\n",
      "5\n",
      "1s\n",
      "3\n",
      "1s\n",
      "1\n",
      "3s\n",
      "5\n",
      "3s\n",
      "4\n",
      "1s\n",
      "1\n",
      "4s\n",
      "4\n",
      "1s\n",
      "2\n",
      "3s\n",
      "2\n",
      "1s\n",
      "1\n",
      "4s\n",
      "3\n",
      "4s\n",
      "4\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "1s\n",
      "1\n",
      "1s\n",
      "1\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "3s\n",
      "3\n",
      "1s\n",
      "5\n",
      "3s\n",
      "4\n",
      "3s\n",
      "2\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "3s\n",
      "1\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "1s\n",
      "1\n",
      "3s\n",
      "4\n",
      "1s\n",
      "2\n",
      "3s\n",
      "2\n",
      "5s\n",
      "4\n",
      "3s\n",
      "3\n",
      "1s\n",
      "1\n",
      "4s\n",
      "5\n",
      "3s\n",
      "1\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "5s\n",
      "4\n",
      "4s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "3\n",
      "5s\n",
      "5\n",
      "3s\n",
      "4\n",
      "3s\n",
      "5\n",
      "4s\n",
      "4\n",
      "1s\n",
      "2\n",
      "5s\n",
      "4\n",
      "3s\n",
      "1\n",
      "3s\n",
      "5\n",
      "3s\n",
      "5\n",
      "4s\n",
      "5\n",
      "3s\n",
      "4\n",
      "3s\n",
      "4\n",
      "4s\n",
      "5\n",
      "3s\n",
      "5\n",
      "3s\n",
      "3\n",
      "4s\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9ad64d8a01b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m's'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stars\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#print js\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9ad64d8a01b0>\u001b[0m in \u001b[0;36msentenceScore\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentenceScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vader_lexicon.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yili/anaconda/lib/python2.7/site-packages/nltk/sentiment/vader.pyc\u001b[0m in \u001b[0;36mpolarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mvalence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0msentitext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentiText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;31m#text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yili/anaconda/lib/python2.7/site-packages/nltk/sentiment/vader.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words_and_emoticons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;31m# doesn't separate words from\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# adjacent punctuation (keeps emoticons & contractions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yili/anaconda/lib/python2.7/site-packages/nltk/sentiment/vader.pyc\u001b[0m in \u001b[0;36m_words_and_emoticons\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpunct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPUNC_LIST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mpword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpunct\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "import json\n",
    "import nltk.sentiment.util as nsu\n",
    "import nltk.sentiment.vader as nsv\n",
    "from pprint import pprint\n",
    "\n",
    "def halfRound(x):\n",
    "    if(x >= 4):\n",
    "        return 5\n",
    "    elif(x >= 3 and x < 3.5):\n",
    "        return 3\n",
    "    elif(x > 3.5 and x < 4):\n",
    "        return 4\n",
    "    elif(x > 2 and x < 3):\n",
    "        return 2\n",
    "    elif(x < 2):\n",
    "        return 1\n",
    "    return x\n",
    "    \n",
    "def sentenceScore(sentence):\n",
    "    analyzer = nltk.sentiment.vader.SentimentIntensityAnalyzer(lexicon_file='vader_lexicon.txt')\n",
    "    s = analyzer.polarity_scores(sentence)\n",
    "    neg = float(s['neg'])\n",
    "    pos = float(s['pos'])\n",
    "    score = 0\n",
    "    if neg > pos:\n",
    "        score = 5 - ((neg - pos) + 1) * 3\n",
    "    elif neg == pos:\n",
    "        return 3\n",
    "    else:\n",
    "        score = ((pos - neg) + 1) * 3\n",
    "    return halfRound(score);\n",
    "\"\"\"\n",
    "{\n",
    "    'type': 'review',\n",
    "    'business_id': (encrypted business id),\n",
    "    'user_id': (encrypted user id),\n",
    "    'stars': (star rating, rounded to half-stars),\n",
    "    'text': (review text),\n",
    "    'date': (date, formatted like '2012-03-14'),\n",
    "    'votes': {(vote type): (count)},\n",
    "}\n",
    "nltk.sentiment.util.demo_liu_hu_lexicon(sentence, plot=False)\n",
    "\"\"\"\n",
    "\n",
    "my_dict = {}\n",
    "file_name=\"yelp_academic_dataset_review.json\"\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        js = json.loads(line)\n",
    "        \n",
    "        \n",
    "        print str(sentenceScore(js[\"text\"])) + 's'\n",
    "        print js[\"stars\"]\n",
    "        #print js\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just some testing code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.188\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "import json\n",
    "import nltk.sentiment.util as nsu\n",
    "import nltk.sentiment.vader as nsv\n",
    " \n",
    "def halfRound(x):\n",
    "    if x % 1 >= 0.5:\n",
    "        return round(x) \n",
    "    else:\n",
    "        return round(x - 0.5)\n",
    "    \n",
    "def sentenceScore(sentence):\n",
    "    analyzer = nltk.sentiment.vader.SentimentIntensityAnalyzer(lexicon_file='vader_lexicon.txt')\n",
    "    s = analyzer.polarity_scores(sentence)\n",
    "    neg = float(s['neg'])\n",
    "    pos = float(s['pos'])\n",
    "    print neg\n",
    "    print pos\n",
    "    score = 0\n",
    "    if neg > pos:\n",
    "        score = 5 - ((neg - pos) + 1) * 3\n",
    "    elif neg == pos:\n",
    "        return 3\n",
    "    else:\n",
    "        score = ((pos - neg) + 1) * 3\n",
    "    return halfRound(score)\n",
    "    \n",
    "\n",
    "\n",
    "sentence = \"i have only had this camera for one full day and i have to say that it is wonderful\"\n",
    "print sentenceScore(sentence)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization and short (detailed) analysis. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at Business Objects. Try to find culinary districts in Las Vegas. These are characterized by closeness and similarity of restaurants. Use the \"longitude\" and \"latitude\" to cluster closeness. Use \"categories\" and \"attributes\" to cluster for similarity.\n",
    "\n",
    "Find clusters using the 3 different techniques we discussed in class: k-means++, hierarchical, and GMM. Explain your data representation and how you determined certain parameters (for example, the number of clusters in k-means++). **(30 pts)**\n",
    "\n",
    "Things you may want to consider:\n",
    "1. The spatial coordinates and restaurant categories/attributes have different units of scale. Your results could be arbitrarily skewed if you don't incorporate some scaling.\n",
    "2. Some restaurant types are inherently more common than others. For example, there are probably lots of \"pizza\" restaurants. You may want to normalize your vectors so that you don't end up with only clusters of \"pizza\" restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your clusters using each technique. Label your clusters. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's detect outliers. These are the ones who are the farthest from the centroids of their clusters. Track them down and describe any interesting observations that you can make. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a short (detailed) analysis comparing the 3 techniques. **(10 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
