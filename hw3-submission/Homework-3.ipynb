{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Hotel Ratings on Tripadvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will focus on practicing two techniques: web scraping and regression. For the first part, we will get some basic information for each hotel in Boston. Then, we will fit a regression model on this information and try to analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 1 (30 pts)**\n",
    "\n",
    "We will scrape the data using Beautiful Soup. For each hotel that our search returns, we will get the information below.\n",
    "\n",
    "![Information to be scraped](hotel_info.png)\n",
    "\n",
    "Of course, feel free to collect even more data if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "url_list = []\n",
    "page = urllib2.urlopen('https://www.tripadvisor.com/Hotels-g60745-Boston_Massachusetts-Hotels.html').read()\n",
    "soup = BeautifulSoup(page, \"lxml\")\n",
    "list = soup.findAll('div', attrs = {'class':'prw_rup prw_common_short_cell_thumbnail'})\n",
    "myNext = soup.findAll('a', attrs = {'class':'nav next ui_button primary taLnk'})\n",
    "for s in list:\n",
    "    tmp = BeautifulSoup(str(s),\"lxml\")\n",
    "    tmpList = tmp.findAll('a', href=True)\n",
    "    for element in tmpList:\n",
    "        arr = str(element).split(' ')\n",
    "        tmp_arr = str(arr[1]).split('=')\n",
    "        url = str(tmp_arr[1])[2:-1]\n",
    "        url_list.append(url)\n",
    "        \n",
    "while len(myNext) == 1:\n",
    "    nextTmp = myNext[0]\n",
    "    st = nextTmp.get('href')\n",
    "    url = 'https://www.tripadvisor.com' + st\n",
    "    page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    list = soup.findAll('div', attrs = {'class':'prw_rup prw_common_short_cell_thumbnail'})\n",
    "    myNext = soup.findAll('a', attrs = {'class':'nav next ui_button primary taLnk'})\n",
    "    for s in list:\n",
    "        tmp = BeautifulSoup(str(s),\"lxml\")\n",
    "        tmpList = tmp.findAll('a', href=True)\n",
    "        for element in tmpList:\n",
    "            arr = str(element).split(' ')\n",
    "            tmp_arr = str(arr[1]).split('=')\n",
    "            url = str(tmp_arr[1])[2:-1]\n",
    "            url_list.append(url)\n",
    "        \n",
    "url_list = set(url_list)\n",
    "target_rating = []\n",
    "count = 0;\n",
    "review_url = []\n",
    "for url in url_list:\n",
    "    url = 'https://www.tripadvisor.com/' + url\n",
    "    page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    innerBubble = soup.findAll('div', attrs = {'class':'reviewSelector'})\n",
    "    tmp =  BeautifulSoup(str(innerBubble[0]), \"lxml\")\n",
    "    tmp_page = tmp.findAll('div', attrs = {'class':'quote'})\n",
    "    div = BeautifulSoup(str(tmp_page), \"lxml\")\n",
    "    div_html = BeautifulSoup(str(div), \"lxml\")\n",
    "    buff = div_html.findAll('a', href = True)\n",
    "    #buff[0]['href']\n",
    "    if(len(buff) == 1):\n",
    "        review_url.append(buff[0]['href']);\n",
    "        \n",
    "for url in review_url:\n",
    "    url = 'https://www.tripadvisor.com' + url\n",
    "    page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    element = soup.findAll('div', attrs = {'class':'col rating '})\n",
    "    element = BeautifulSoup(str(element), \"lxml\")\n",
    "    temp = element.findAll('li')\n",
    "    temp = BeautifulSoup(str(temp), \"lxml\")\n",
    "    span = temp.findAll('span', attrs={'class': None})\n",
    "    test = str(span).split('<span>')\n",
    "    \n",
    "    num1 = int(test[1].replace(\",\", \"\"))\n",
    "    num2 = int(test[4].replace(\",\", \"\"))\n",
    "    num3 = int(test[7].replace(\",\", \"\"))\n",
    "    num4 = int(test[10].replace(\",\", \"\"))\n",
    "    num5 = int(test[13].replace(\",\", \"\"))\n",
    "    \n",
    "    y = num1 * 5 + num2 + 4 + num3 * 3 + num4 * 2 + num5 * 1\n",
    "    y = float(y) / (num1 + num2 + num3 + num4 + num5)\n",
    "    y = round(y, 2)\n",
    "    target_rating.append(y)\n",
    "\n",
    "y_df = pd.DataFrame(target_rating)\n",
    "y_df.to_csv('target.csv')  \n",
    "\n",
    "df = pd.DataFrame(review_url)\n",
    "df.to_csv('url.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.951, 4.872, 4.361, 4.205, 4.467, 3.954]\n",
      "[4.029, 4.451, 4.403, 4.403, 4.61, 4.029]\n",
      "[3.747, 4.747, 3.973, 3.853, 4.4, 3.747]\n",
      "[4.054, 4.44, 4.021, 3.831, 4.443, 4.054]\n",
      "[4.207, 4.872, 4.079, 3.879, 4.534, 4.207]\n",
      "[3.0, 2.674, 3.196, 3.043, 3.478, 3.0]\n",
      "[4.007, 4.688, 4.366, 4.359, 4.544, 4.01]\n",
      "[3.838, 4.531, 4.494, 4.351, 4.649, 3.838]\n",
      "[4.346, 4.843, 4.738, 4.738, 4.847, 4.346]\n",
      "[3.805, 4.857, 4.405, 4.271, 4.471, 3.805]\n",
      "[4.023, 4.761, 4.258, 4.453, 4.638, 4.023]\n",
      "[4.113, 4.374, 4.247, 3.725, 4.341, 4.115]\n",
      "[3.743, 4.515, 4.245, 4.238, 4.557, 3.743]\n",
      "[3.794, 4.008, 4.293, 4.289, 4.63, 3.794]\n",
      "[3.913, 4.614, 4.157, 4.003, 4.205, 3.913]\n",
      "[3.572, 3.736, 4.049, 3.974, 4.15, 3.576]\n",
      "[4.085, 4.786, 4.201, 4.147, 4.46, 4.085]\n",
      "[4.521, 4.884, 4.664, 4.63, 4.925, 4.534]\n",
      "[4.256, 4.911, 4.804, 4.686, 4.894, 4.256]\n",
      "[4.401, 4.914, 4.58, 4.572, 4.781, 4.401]\n",
      "[4.067, 3.351, 3.937, 4.138, 4.212, 4.074]\n",
      "[4.003, 4.489, 4.383, 4.391, 4.574, 4.003]\n",
      "[4.186, 4.834, 4.563, 4.523, 4.727, 4.186]\n",
      "[4.12, 4.827, 4.363, 4.157, 4.59, 4.12]\n",
      "[4.138, 4.653, 4.524, 4.488, 4.755, 4.138]\n",
      "[3.979, 3.705, 3.993, 3.849, 4.151, 3.979]\n",
      "[3, 3, 3, 3, 3, 3]\n",
      "[4.212, 4.802, 4.586, 4.646, 4.79, 4.212]\n",
      "[4.027, 4.827, 4.276, 4.103, 4.539, 4.03]\n",
      "[3.64, 4.807, 4.188, 4.012, 4.396, 3.64]\n",
      "[3.488, 4.488, 3.977, 3.884, 4.395, 3.488]\n",
      "[3.188, 3.274, 3.418, 3.466, 3.673, 3.191]\n",
      "[4.407, 4.097, 4.342, 4.529, 4.663, 4.407]\n",
      "[4.142, 4.878, 4.791, 4.77, 4.861, 4.142]\n",
      "[4.647, 4.869, 4.821, 4.85, 4.919, 4.652]\n",
      "[4.421, 4.766, 4.086, 3.965, 4.335, 4.419]\n",
      "[4.542, 4.75, 4.639, 4.778, 4.861, 4.542]\n",
      "[3.975, 3.589, 4.133, 4.215, 4.348, 3.975]\n",
      "[3.47, 4.517, 3.695, 2.94, 3.384, 3.47]\n",
      "[3, 3, 3, 3, 3, 3]\n",
      "[3.904, 4.018, 4.229, 4.272, 4.386, 3.904]\n",
      "[4.667, 4.556, 4.556, 4.556, 4.778, 4.667]\n",
      "[3.77, 4.8, 4.214, 4.517, 4.594, 3.773]\n",
      "[4.2, 4.573, 4.518, 4.764, 4.818, 4.2]\n",
      "[4.249, 4.873, 4.707, 4.721, 4.832, 4.249]\n",
      "[3.355, 4.366, 3.317, 2.823, 3.575, 3.355]\n",
      "[4.061, 4.503, 4.245, 4.0, 4.515, 4.061]\n",
      "[3.581, 3.964, 3.874, 3.877, 4.107, 3.581]\n",
      "[4.505, 4.787, 4.792, 4.82, 4.894, 4.505]\n",
      "[3.808, 4.562, 4.218, 4.237, 4.511, 3.808]\n",
      "[2.429, 2.714, 3.0, 2.714, 2.857, 2.429]\n",
      "[3.831, 3.915, 4.443, 4.249, 4.522, 3.831]\n",
      "[4.176, 4.748, 4.527, 4.429, 4.668, 4.176]\n",
      "[4.665, 4.173, 4.569, 4.675, 4.853, 4.665]\n",
      "[3.747, 4.667, 4.202, 4.135, 4.377, 3.747]\n",
      "[3, 3, 3, 3, 3, 3]\n",
      "[4.286, 4.797, 4.729, 4.684, 4.797, 4.286]\n",
      "[4.183, 4.831, 4.257, 4.198, 4.492, 4.183]\n",
      "[3.933, 4.267, 4.246, 4.243, 4.337, 3.932]\n",
      "[3.817, 4.898, 4.328, 4.228, 4.557, 3.817]\n",
      "[3.713, 4.83, 4.071, 3.601, 4.174, 3.716]\n",
      "[3.178, 4.568, 3.628, 3.092, 3.775, 3.178]\n",
      "[4.099, 4.68, 4.149, 3.789, 4.274, 4.102]\n",
      "[3.691, 3.784, 3.462, 3.174, 3.462, 3.691]\n",
      "[3.107, 3.305, 3.186, 3.44, 3.562, 3.107]\n",
      "[4.228, 4.852, 4.399, 4.601, 4.641, 4.228]\n",
      "[3.773, 4.653, 4.231, 4.166, 4.36, 3.778]\n",
      "[4.177, 4.385, 4.403, 4.375, 4.53, 4.177]\n",
      "[3.885, 4.27, 4.305, 3.993, 4.466, 3.885]\n",
      "[3.894, 4.865, 4.243, 4.325, 4.538, 3.894]\n",
      "[3.9, 4.544, 3.694, 3.556, 3.763, 3.9]\n",
      "[3.559, 4.426, 3.709, 4.017, 4.223, 3.56]\n",
      "[4.394, 4.869, 4.539, 4.488, 4.704, 4.394]\n",
      "[4.344, 4.83, 4.452, 4.589, 4.506, 4.344]\n",
      "[3.575, 4.281, 3.603, 3.562, 3.938, 3.575]\n",
      "[3.969, 4.722, 4.435, 4.386, 4.543, 3.969]\n",
      "[3.829, 4.441, 3.784, 3.488, 3.902, 3.829]\n",
      "[3.066, 4.123, 3.11, 2.829, 3.586, 3.066]\n",
      "[4.016, 4.89, 4.533, 4.427, 4.691, 4.016]\n",
      "[4.17, 4.797, 4.556, 4.455, 4.597, 4.17]\n",
      "[4.0, 4.573, 4.561, 4.53, 4.738, 4.0]\n",
      "[[3.951, 4.872, 4.361, 4.205, 4.467, 3.954], [4.029, 4.451, 4.403, 4.403, 4.61, 4.029], [3.747, 4.747, 3.973, 3.853, 4.4, 3.747], [4.054, 4.44, 4.021, 3.831, 4.443, 4.054], [4.207, 4.872, 4.079, 3.879, 4.534, 4.207], [3.0, 2.674, 3.196, 3.043, 3.478, 3.0], [4.007, 4.688, 4.366, 4.359, 4.544, 4.01], [3.838, 4.531, 4.494, 4.351, 4.649, 3.838], [4.346, 4.843, 4.738, 4.738, 4.847, 4.346], [3.805, 4.857, 4.405, 4.271, 4.471, 3.805], [4.023, 4.761, 4.258, 4.453, 4.638, 4.023], [4.113, 4.374, 4.247, 3.725, 4.341, 4.115], [3.743, 4.515, 4.245, 4.238, 4.557, 3.743], [3.794, 4.008, 4.293, 4.289, 4.63, 3.794], [3.913, 4.614, 4.157, 4.003, 4.205, 3.913], [3.572, 3.736, 4.049, 3.974, 4.15, 3.576], [4.085, 4.786, 4.201, 4.147, 4.46, 4.085], [4.521, 4.884, 4.664, 4.63, 4.925, 4.534], [4.256, 4.911, 4.804, 4.686, 4.894, 4.256], [4.401, 4.914, 4.58, 4.572, 4.781, 4.401], [4.067, 3.351, 3.937, 4.138, 4.212, 4.074], [4.003, 4.489, 4.383, 4.391, 4.574, 4.003], [4.186, 4.834, 4.563, 4.523, 4.727, 4.186], [4.12, 4.827, 4.363, 4.157, 4.59, 4.12], [4.138, 4.653, 4.524, 4.488, 4.755, 4.138], [3.979, 3.705, 3.993, 3.849, 4.151, 3.979], [3, 3, 3, 3, 3, 3], [4.212, 4.802, 4.586, 4.646, 4.79, 4.212], [4.027, 4.827, 4.276, 4.103, 4.539, 4.03], [3.64, 4.807, 4.188, 4.012, 4.396, 3.64], [3.488, 4.488, 3.977, 3.884, 4.395, 3.488], [3.188, 3.274, 3.418, 3.466, 3.673, 3.191], [4.407, 4.097, 4.342, 4.529, 4.663, 4.407], [4.142, 4.878, 4.791, 4.77, 4.861, 4.142], [4.647, 4.869, 4.821, 4.85, 4.919, 4.652], [4.421, 4.766, 4.086, 3.965, 4.335, 4.419], [4.542, 4.75, 4.639, 4.778, 4.861, 4.542], [3.975, 3.589, 4.133, 4.215, 4.348, 3.975], [3.47, 4.517, 3.695, 2.94, 3.384, 3.47], [3, 3, 3, 3, 3, 3], [3.904, 4.018, 4.229, 4.272, 4.386, 3.904], [4.667, 4.556, 4.556, 4.556, 4.778, 4.667], [3.77, 4.8, 4.214, 4.517, 4.594, 3.773], [4.2, 4.573, 4.518, 4.764, 4.818, 4.2], [4.249, 4.873, 4.707, 4.721, 4.832, 4.249], [3.355, 4.366, 3.317, 2.823, 3.575, 3.355], [4.061, 4.503, 4.245, 4.0, 4.515, 4.061], [3.581, 3.964, 3.874, 3.877, 4.107, 3.581], [4.505, 4.787, 4.792, 4.82, 4.894, 4.505], [3.808, 4.562, 4.218, 4.237, 4.511, 3.808], [2.429, 2.714, 3.0, 2.714, 2.857, 2.429], [3.831, 3.915, 4.443, 4.249, 4.522, 3.831], [4.176, 4.748, 4.527, 4.429, 4.668, 4.176], [4.665, 4.173, 4.569, 4.675, 4.853, 4.665], [3.747, 4.667, 4.202, 4.135, 4.377, 3.747], [3, 3, 3, 3, 3, 3], [4.286, 4.797, 4.729, 4.684, 4.797, 4.286], [4.183, 4.831, 4.257, 4.198, 4.492, 4.183], [3.933, 4.267, 4.246, 4.243, 4.337, 3.932], [3.817, 4.898, 4.328, 4.228, 4.557, 3.817], [3.713, 4.83, 4.071, 3.601, 4.174, 3.716], [3.178, 4.568, 3.628, 3.092, 3.775, 3.178], [4.099, 4.68, 4.149, 3.789, 4.274, 4.102], [3.691, 3.784, 3.462, 3.174, 3.462, 3.691], [3.107, 3.305, 3.186, 3.44, 3.562, 3.107], [4.228, 4.852, 4.399, 4.601, 4.641, 4.228], [3.773, 4.653, 4.231, 4.166, 4.36, 3.778], [4.177, 4.385, 4.403, 4.375, 4.53, 4.177], [3.885, 4.27, 4.305, 3.993, 4.466, 3.885], [3.894, 4.865, 4.243, 4.325, 4.538, 3.894], [3.9, 4.544, 3.694, 3.556, 3.763, 3.9], [3.559, 4.426, 3.709, 4.017, 4.223, 3.56], [4.394, 4.869, 4.539, 4.488, 4.704, 4.394], [4.344, 4.83, 4.452, 4.589, 4.506, 4.344], [3.575, 4.281, 3.603, 3.562, 3.938, 3.575], [3.969, 4.722, 4.435, 4.386, 4.543, 3.969], [3.829, 4.441, 3.784, 3.488, 3.902, 3.829], [3.066, 4.123, 3.11, 2.829, 3.586, 3.066], [4.016, 4.89, 4.533, 4.427, 4.691, 4.016], [4.17, 4.797, 4.556, 4.455, 4.597, 4.17], [4.0, 4.573, 4.561, 4.53, 4.738, 4.0]]\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "url = []\n",
    "allVector = []\n",
    "with open('url.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        arr = str(row).split('/')\n",
    "        url.append(arr[1][:-2])\n",
    "        \n",
    "driver = webdriver.Firefox()\n",
    "for element in url:\n",
    "    value = 0\n",
    "    location = 0\n",
    "    s_quality = 0\n",
    "    rooms = 0\n",
    "    clean = 0\n",
    "    service = 0\n",
    "    total = 0\n",
    "    c = 0\n",
    "    link = 'https://www.tripadvisor.com/' + element\n",
    "    page = urllib2.urlopen(link).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    current = soup.findAll('span', attrs = {'class':'pageNum current'})\n",
    "    my_next = soup.findAll('a', attrs = {'class':'nav next rndBtn ui_button primary taLnk'})\n",
    "    innerBubble = soup.findAll('div', attrs = {'class':'innerBubble'})\n",
    "    buff = BeautifulSoup(str(innerBubble), \"lxml\")\n",
    "    recommend = buff.findAll('ul', attrs = {'class':'recommend'})\n",
    "    \n",
    "    for e in recommend:\n",
    "        tmpSoup = BeautifulSoup(str(e), \"lxml\")\n",
    "        tmpRec = tmpSoup.findAll('li', attrs = {'class':'recommend-answer'})\n",
    "        if len(tmpRec) == 6:\n",
    "            total = total + 1\n",
    "            tmp0 = str(tmpRec[0])\n",
    "            html0 = BeautifulSoup(tmp0, \"lxml\")\n",
    "            current0 = html0.findAll('img', alt = True)\n",
    "            value = value + int(current0[0].get('alt')[:1])\n",
    "            \n",
    "            tmp1 = str(tmpRec[1])\n",
    "            html1 = BeautifulSoup(tmp1, \"lxml\")\n",
    "            current1 = html1.findAll('img', alt = True)\n",
    "            location = location + int(current1[0].get('alt')[:1])\n",
    "            \n",
    "            tmp2 = str(tmpRec[2])\n",
    "            html2 = BeautifulSoup(tmp2, \"lxml\")\n",
    "            current2 = html2.findAll('img', alt = True)\n",
    "            s_quality = s_quality + int(current2[0].get('alt')[:1])\n",
    "            \n",
    "            tmp3 = str(tmpRec[3])\n",
    "            html3 = BeautifulSoup(tmp3, \"lxml\")\n",
    "            current3 = html3.findAll('img', alt = True)\n",
    "            rooms = rooms + int(current3[0].get('alt')[:1])\n",
    "            \n",
    "            tmp4 = str(tmpRec[4])\n",
    "            html4 = BeautifulSoup(tmp4, \"lxml\")\n",
    "            current4 = html4.findAll('img', alt = True)\n",
    "            clean = clean + int(current4[0].get('alt')[:1])\n",
    "            \n",
    "            tmp5 = str(tmpRec[5])\n",
    "            html5 = BeautifulSoup(tmp5, \"lxml\")\n",
    "            current5 = html5.findAll('img', alt = True)\n",
    "            service = service + int(current5[0].get('alt')[:1])\n",
    "    url_link = link\n",
    "    html_page = urllib2.urlopen(url_link).read()\n",
    "    bs = BeautifulSoup(html_page, \"lxml\")\n",
    "    count = 0\n",
    "    next_page = bs.findAll('a', attrs = {'class':'nav next rndBtn ui_button primary taLnk'})\n",
    "    \n",
    "    template = url_link[:-16] + 'or'\n",
    "    html_page = urllib2.urlopen(url_link).read()\n",
    "    bs = BeautifulSoup(html_page, \"lxml\")\n",
    "    count = 0\n",
    "    next_div = bs.findAll('a', attrs = {'class':'nav next rndBtn ui_button primary taLnk'})\n",
    "    current = int(next_div[0].get('href')[-1:])\n",
    "    while count < 200:\n",
    "        url = template + str(current)\n",
    "        driver.get(url)\n",
    "    \n",
    "        page_source = driver.page_source\n",
    "        soup_2 = BeautifulSoup(page_source, \"lxml\")\n",
    "        rating = soup_2.findAll('ul', attrs = {'class':'recommend'})\n",
    "        for r in rating:\n",
    "            ratingSoup = BeautifulSoup(str(r), \"lxml\")\n",
    "            ratingRec = ratingSoup.findAll('li', attrs = {'class':'recommend-answer'})\n",
    "            \n",
    "            if len(ratingRec) == 6:\n",
    "                total = total + 1\n",
    "                rating_tmp0 = str(ratingRec[0])\n",
    "                rating_html0 = BeautifulSoup(rating_tmp0, \"lxml\")\n",
    "                rating_current0 = rating_html0.findAll('img', alt = True)\n",
    "                value = value + int(rating_current0[0].get('alt')[:1])\n",
    "                \n",
    "                rating_tmp1 = str(ratingRec[1])\n",
    "                rating_html1 = BeautifulSoup(rating_tmp1, \"lxml\")\n",
    "                rating_current1 = rating_html1.findAll('img', alt = True)\n",
    "                location = location + int(rating_current1[0].get('alt')[:1])\n",
    "                \n",
    "                rating_tmp2 = str(ratingRec[2])\n",
    "                rating_html2 = BeautifulSoup(rating_tmp2, \"lxml\")\n",
    "                rating_current2 = rating_html2.findAll('img', alt = True)\n",
    "                s_quality = s_quality + int(rating_current2[0].get('alt')[:1])\n",
    "                \n",
    "                rating_tmp3 = str(ratingRec[3])\n",
    "                rating_html3 = BeautifulSoup(rating_tmp3, \"lxml\")\n",
    "                rating_current3 = rating_html3.findAll('img', alt = True)\n",
    "                rooms = rooms + int(rating_current3[0].get('alt')[:1])\n",
    "                \n",
    "                rating_tmp4 = str(ratingRec[4])\n",
    "                rating_html4 = BeautifulSoup(rating_tmp4, \"lxml\")\n",
    "                rating_current4 = rating_html4.findAll('img', alt = True)\n",
    "                clean = clean + int(rating_current4[0].get('alt')[:1])\n",
    "                \n",
    "                rating_tmp5 = str(ratingRec[5])\n",
    "                rating_html5 = BeautifulSoup(rating_tmp5, \"lxml\")\n",
    "                rating_current5 = rating_html5.findAll('img', alt = True)\n",
    "                service = service + int(rating_current0[0].get('alt')[:1])\n",
    "        current = current + 6\n",
    "        count = count + 1\n",
    "    vector = [value, location, s_quality, rooms, clean, service]\n",
    "    #print vector\n",
    "    #print total\n",
    "    if(total == 0):\n",
    "        v = [3, 3, 3, 3, 3, 3]\n",
    "    else:\n",
    "        vc = [float(x) / total for x in vector]\n",
    "        v = [ round(elem, 3) for elem in vc ]\n",
    "    \n",
    "    print v\n",
    "    allVector.append(v)\n",
    "print allVector\n",
    "print len(allVector)\n",
    "\n",
    "data_x = pd.DataFrame(allVector)\n",
    "data_x.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = []\n",
    "allVector = []\n",
    "with open('url.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        arr = str(row).split('/')\n",
    "        url.append(arr[1][:-2])\n",
    "        \n",
    "travel_types = []\n",
    "for element in url:\n",
    "    url = 'https://www.tripadvisor.com/' + element\n",
    "    page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    div = soup.findAll('div', attrs = {'class':'col segment '})\n",
    "    travel_type = BeautifulSoup(str(div), \"lxml\")\n",
    "    li = travel_type.findAll('li')\n",
    "    vector = []\n",
    "    for a in li:\n",
    "        tmp = BeautifulSoup(str(a), \"lxml\")\n",
    "        buff = tmp.findAll('span', attrs={'class': None})\n",
    "        num = BeautifulSoup(str(buff[0])).find(text=True)\n",
    "        vector.append(int(num[1:-1].replace(\",\", \"\")))\n",
    "    travel_types.append(vector)\n",
    "\n",
    "print len(travel_types)\n",
    "df = pd.DataFrame(travel_types)\n",
    "df.to_csv('travel_type.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract omni-parker\n",
    "#https://www.tripadvisor.com/ShowUserReviews-g60745-d89599-r359484392-Omni_Parker_House-Boston_Massachusetts.html#or7\n",
    "import urllib2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "count = 0\n",
    "current = 7\n",
    "review_list = []\n",
    "template = \"https://www.tripadvisor.com/ShowUserReviews-g60745-d89599-r359484392-Omni_Parker_House-Boston_Massachusetts.html#or\"\n",
    "while count < 800:\n",
    "    url = template + str(current)\n",
    "    #print url\n",
    "    driver.get(url)\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    soup_2 = BeautifulSoup(page_source, \"lxml\")\n",
    "    review_block = soup_2.findAll('div', attrs = {'class':'  reviewSelector '})\n",
    "    review_block.pop(0)\n",
    "    for r in review_block:\n",
    "        reviewId = r.get('id')\n",
    "        ratingSoup = BeautifulSoup(str(r), \"lxml\")\n",
    "        ratingRec = ratingSoup.findAll('li', attrs = {'class':'recommend-answer'})\n",
    "        for e in ratingRec:\n",
    "            rating_tmp0 = str(e)\n",
    "            rating_html0 = BeautifulSoup(rating_tmp0, \"lxml\")\n",
    "            rating_current0 = rating_html0.findAll('img', alt = True)\n",
    "            current0 = rating_html0.findAll('img', alt = True)\n",
    "            type0 = rating_html0.findAll('div', attrs = {'class':'recommend-description'})\n",
    "            value = int(current0[0].get('alt')[:1])\n",
    "            value_tmp = str(value)\n",
    "            st = reviewId + \":\" + type0[0].string + \":\" + value_tmp\n",
    "            review_list.append(st)\n",
    "    count = count + 1\n",
    "    current = current + 6\n",
    "\n",
    "f = open('output.txt', 'w')\n",
    "for element in review_list:\n",
    "    f.write(element+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# README: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I extracted data and saved them in CSV files instead of printing them out. Please take a look at the csv file in the Git repository. The 'target.csv' is the overall rating of each hotel. I extrated data of 'traveller rating' on the web page and aggregate them to caculate the overall rating of each hotel. The 'clean_data.csv' contains ratings of attributes(clealiness, location, etc) that we want to use for further analysis, and a vector contains value of those 6 attributes is representing a hotel.To get each of the attributes(clealiness, location, etc), I retrived almost half of all reviews of each hotel and sum them up and devide by total review number. So the rate for each atttribute is in the range of 1 to 5. The 'travel_type.csv' file contains data of travel type of each restaurant(families, couples, business, etc). 'Output.txt' is the file contains the reviews of Omin parker hotel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 2 (20 pts) **\n",
    "\n",
    "Now, we will use regression to analyze this information. First, we will fit a linear regression model that predicts the average rating. For example, for the hotel above, the average rating is\n",
    "\n",
    "$$ \\text{AVG_SCORE} = \\frac{1*31 + 2*33 + 3*98 + 4*504 + 5*1861}{2527}$$\n",
    "\n",
    "Use the model to analyze the important factors that decide the $\\text{AVG_SCORE}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.985\n",
      "Model:                            OLS   Adj. R-squared:                  0.984\n",
      "Method:                 Least Squares   F-statistic:                     814.9\n",
      "Date:                Tue, 29 Mar 2016   Prob (F-statistic):           3.88e-66\n",
      "Time:                        23:50:42   Log-Likelihood:                -40.202\n",
      "No. Observations:                  81   AIC:                             92.40\n",
      "Df Residuals:                      75   BIC:                             106.8\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -4.2418     19.692     -0.215      0.830       -43.470    34.987\n",
      "x2             0.0744      0.145      0.512      0.610        -0.215     0.364\n",
      "x3             0.8921      0.425      2.098      0.039         0.045     1.739\n",
      "x4             1.0666      0.308      3.463      0.001         0.453     1.680\n",
      "x5            -1.2571      0.404     -3.115      0.003        -2.061    -0.453\n",
      "x6             4.3028     19.700      0.218      0.828       -34.942    43.548\n",
      "==============================================================================\n",
      "Omnibus:                        0.826   Durbin-Watson:                   2.326\n",
      "Prob(Omnibus):                  0.662   Jarque-Bera (JB):                0.935\n",
      "Skew:                           0.177   Prob(JB):                        0.627\n",
      "Kurtosis:                       2.610   Cond. No.                     6.20e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.2e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Confidence Intervals: [[-43.47019719  34.98655506]\n",
      " [ -0.21499568   0.36375053]\n",
      " [  0.04507411   1.73916617]\n",
      " [  0.45305402   1.68007172]\n",
      " [ -2.06095213  -0.45326768]\n",
      " [-34.94222986  43.54784377]]\n",
      "Parameters: [-4.24182106  0.07437743  0.89212014  1.06656287 -1.25710991  4.30280695]\n",
      "('Coefficients: \\n', array([ 25.02110862,   0.38615671,   0.59434466,   1.28706652,\n",
      "        -1.25318849, -25.38200164]))\n",
      "Residual sum of squares: 0.24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "import pandas as pd\n",
    "import sklearn.cross_validation as cross_validation\n",
    "import statsmodels.api as sm\n",
    "#value, location, sleep quality, rooms, cleanliness, service\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "y = pd.read_csv('target.csv')\n",
    "x = pd.read_csv('clean_data.csv')\n",
    "target = []\n",
    "training_set = []\n",
    "\n",
    "for index, row in y.iterrows():\n",
    "    target.append(row[1])\n",
    "\n",
    "target_y = [ round(elem, 2) for elem in target ]\n",
    "#row 4\n",
    "for index, row in x.iterrows():\n",
    "    tmp = [row[1], row[2], row[3], row[4], row[5], row[6]]\n",
    "    v = [ round(elem, 2) for elem in tmp ]\n",
    "    training_set.append(v)\n",
    "    \n",
    "ax = np.array(training_set)\n",
    "target_value = np.array(target_y)\n",
    "\n",
    "model = sm.OLS(target_value, ax)\n",
    "results = model.fit()\n",
    "print results.summary()\n",
    "print \"Confidence Intervals:\", results.conf_int()\n",
    "print \"Parameters:\", results.params\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(ax, target_value, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean square error\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "      % np.mean((regr.predict(X_test) - y_test) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I do it is to elimiate one attribute but keep the other 5 factors in the vector for each time. For example, I will delete 'value' attribute but keep the rest of 5 and see the mean square value. If I ignore 'value', 'clenliness', factors (only delete one for each time), the mean square error increases, this shows they are relatively important factors. And their coefficiencies are bigger than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 3 (30 pts) **\n",
    "\n",
    "Finally, we will use logistic regression to decide if a hotel is _excellent_ or not. We classify a hotel as _excellent_ if more than **60%** of its ratings are 5 stars. This is a binary attribute on which we can fit a logistic regression model. As before, use the model to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = []\n",
    "allVector = []\n",
    "with open('url.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        arr = str(row).split('/')\n",
    "        url.append(arr[1][:-2])\n",
    "        \n",
    "hotel_label = []\n",
    "for link in url:\n",
    "    url = 'https://www.tripadvisor.com/' + link\n",
    "    page = urllib2.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    element = soup.findAll('div', attrs = {'class':'col rating '})\n",
    "    element = BeautifulSoup(str(element), \"lxml\")\n",
    "    temp = element.findAll('li')\n",
    "    temp = BeautifulSoup(str(temp), \"lxml\")\n",
    "    span = temp.findAll('span', attrs={'class': None})\n",
    "    test = str(span).split('<span>')\n",
    "    \n",
    "    num1 = int(test[1].replace(\",\", \"\"))\n",
    "    num2 = int(test[4].replace(\",\", \"\"))\n",
    "    num3 = int(test[7].replace(\",\", \"\"))\n",
    "    num4 = int(test[10].replace(\",\", \"\"))\n",
    "    num5 = int(test[13].replace(\",\", \"\"))\n",
    "    \n",
    "    total = (num1 + num2 + num3 + num4 + num5)\n",
    "    #print num1\n",
    "    if (float(num1) / total) >= 0.60:\n",
    "        hotel_label.append(1)\n",
    "    else:\n",
    "        hotel_label.append(0)\n",
    "print hotel_label\n",
    "df = pd.DataFrame(hotel_label)\n",
    "df.to_csv('hotel_label.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.547459\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   81\n",
      "Model:                          Logit   Df Residuals:                       75\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Tue, 29 Mar 2016   Pseudo R-squ.:                 0.06393\n",
      "Time:                        17:37:43   Log-Likelihood:                -44.344\n",
      "converged:                       True   LL-Null:                       -47.373\n",
      "                                        LLR p-value:                    0.3007\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "x1            67.2571     46.012      1.462      0.144       -22.924   157.439\n",
      "x2            -0.3197      0.653     -0.489      0.625        -1.600     0.961\n",
      "x3            -0.2861      1.146     -0.250      0.803        -2.533     1.961\n",
      "x4             1.9374      1.060      1.827      0.068        -0.141     4.016\n",
      "x5            -0.6001      0.942     -0.637      0.524        -2.447     1.246\n",
      "x6           -68.1893     45.987     -1.483      0.138      -158.322    21.944\n",
      "==============================================================================\n",
      "0.673469387755\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "import pandas as pd\n",
    "import sklearn.cross_validation as cross_validation\n",
    "import statsmodels.api as sm\n",
    "#value, location, sleep quality, rooms, cleanliness, service\n",
    "\n",
    "# Create linear regression object\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "y = pd.read_csv('hotel_label.csv')\n",
    "x = pd.read_csv('clean_data.csv')\n",
    "target = []\n",
    "training_set = []\n",
    "\n",
    "for index, row in y.iterrows():\n",
    "    target.append(row[1])\n",
    "\n",
    "target_y = [ round(elem, 2) for elem in target ]\n",
    "#row 4\n",
    "for index, row in x.iterrows():\n",
    "    tmp = [row[1], row[2], row[3], row[4], row[5], row[6]]\n",
    "    v = [ round(elem, 2) for elem in tmp ]\n",
    "    training_set.append(v)\n",
    "    \n",
    "ax = np.array(training_set)\n",
    "target_value = np.array(target_y)\n",
    "\n",
    "logit = sm.Logit(target_value, ax)\n",
    " \n",
    "# fit the model\n",
    "result = logit.fit() \n",
    "print result.summary()\n",
    "\n",
    "h = .02\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(ax, target_value, test_size=0.6, random_state=0)\n",
    "\n",
    "#print a\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "hypothesis = logreg.predict(X_test)\n",
    "\n",
    "total = len(hypothesis)\n",
    "count = 0\n",
    "for f, b in zip(hypothesis, y_test):\n",
    "    if f == b:\n",
    "        count = count + 1\n",
    "\n",
    "print float(count) / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# README:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by using logistic regression, my prediction has an 67% of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
