{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will look at some posts on Stack Overflow during the year of 2015 and measure the similarity of users by looking at the types of questions they answer. We will also analyze the creation dates of questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working on the notebook, let's make sure that everything is setup properly. You should have downloaded and installed\n",
    "* [Anaconda](https://store.continuum.io/cshop/anaconda/)\n",
    "* [Git](http://git-scm.com/downloads)\n",
    "\n",
    "If you are working from the undergraduate lab (on a linux machine) these are both installed, but you need to follow the instructions [from here](https://github.com/datascience16/lectures/blob/master/Lecture2/Getting-Started.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sample request to retrieve the questions posted on Stack Exchange on the first day of 2015. Documentation of the Stack Exchange API can be found [here](https://api.stackexchange.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "start_time = 1420070400 # 01-01-2015 at 00:00:00\n",
    "end_time   = 1420156800 # 01-02-2015 at 00:00:00\n",
    "\n",
    "response = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100\" +\n",
    "                        \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\n",
    "                        \"&order=asc&sort=creation&site=stackoverflow\")\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates in the Stack Exchange API are in [unix epoch time](https://en.wikipedia.org/wiki/Unix_time). The format for the request string is specified [here](https://api.stackexchange.com/docs/questions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to print the response that Stack Exchange returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to read the raw response. Instead, we need to decode the raw response as JSON and use the `json` library to print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print json.dumps(response.json(), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily see that the response consists of a list of question items. For each of these items, we get information about its attributes such as its `creation_date`, `answer_count`, `owner`, `title`, etc.\n",
    "\n",
    "Notice that has_more is true. To get more items, we can [request the next page](https://api.stackexchange.com/docs/paging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Parsing the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we practice some of the basic Python tools that we learned in class and the powerful string handling methods that Python offers. Our goal is to be able to pick the interesting parts of the response and transform them in a format that will be useful to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's isolate the creation_date in the response. Fill in the rest of the ```print_creation_dates_json()``` function that reads the response and prints the creation dates. Notice that a JSON object is basically a dictionary. **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "def print_creation_dates_json(response):\n",
    "    data = json.loads(response.text)\n",
    "    items = data['items']\n",
    "    for element in items:\n",
    "        #print element['creation_date']\n",
    "        print datetime.datetime.utcfromtimestamp(int(element['creation_date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that calls the ```print_creation_dates_json()``` function to print out all the creation dates of questions posted on the first day in 2015. Please be aware of Stack Exchange's [rate limit](https://api.stackexchange.com/docs/throttle). **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 00:00:58\n",
      "2015-01-01 00:01:43\n",
      "2015-01-01 00:02:32\n",
      "2015-01-01 00:02:57\n",
      "2015-01-01 00:03:31\n",
      "2015-01-01 00:04:01\n",
      "2015-01-01 00:05:03\n",
      "2015-01-01 00:05:27\n",
      "2015-01-01 00:05:34\n",
      "2015-01-01 00:06:17\n",
      "2015-01-01 00:06:41\n",
      "2015-01-01 00:07:28\n",
      "2015-01-01 00:07:39\n",
      "2015-01-01 00:07:46\n",
      "2015-01-01 00:09:28\n",
      "2015-01-01 00:10:05\n",
      "2015-01-01 00:10:29\n",
      "2015-01-01 00:11:43\n",
      "2015-01-01 00:12:02\n",
      "2015-01-01 00:12:55\n",
      "2015-01-01 00:13:04\n",
      "2015-01-01 00:13:32\n",
      "2015-01-01 00:13:50\n",
      "2015-01-01 00:15:40\n",
      "2015-01-01 00:17:11\n",
      "2015-01-01 00:18:50\n",
      "2015-01-01 00:22:16\n",
      "2015-01-01 00:23:14\n",
      "2015-01-01 00:23:50\n",
      "2015-01-01 00:24:28\n",
      "2015-01-01 00:25:07\n",
      "2015-01-01 00:25:29\n",
      "2015-01-01 00:25:39\n",
      "2015-01-01 00:26:42\n",
      "2015-01-01 00:27:01\n",
      "2015-01-01 00:27:54\n",
      "2015-01-01 00:28:49\n",
      "2015-01-01 00:30:43\n",
      "2015-01-01 00:32:22\n",
      "2015-01-01 00:32:34\n",
      "2015-01-01 00:33:17\n",
      "2015-01-01 00:33:50\n",
      "2015-01-01 00:34:15\n",
      "2015-01-01 00:34:41\n",
      "2015-01-01 00:36:50\n",
      "2015-01-01 00:37:18\n",
      "2015-01-01 00:37:47\n",
      "2015-01-01 00:38:05\n",
      "2015-01-01 00:39:37\n",
      "2015-01-01 00:39:39\n",
      "2015-01-01 00:41:42\n",
      "2015-01-01 00:42:04\n",
      "2015-01-01 00:42:56\n",
      "2015-01-01 00:42:59\n",
      "2015-01-01 00:43:17\n",
      "2015-01-01 00:44:15\n",
      "2015-01-01 00:46:09\n",
      "2015-01-01 00:47:53\n",
      "2015-01-01 00:47:56\n",
      "2015-01-01 00:49:12\n",
      "2015-01-01 00:49:43\n",
      "2015-01-01 00:50:25\n",
      "2015-01-01 00:50:55\n",
      "2015-01-01 00:50:56\n",
      "2015-01-01 00:51:32\n",
      "2015-01-01 00:51:50\n",
      "2015-01-01 00:52:04\n",
      "2015-01-01 00:56:27\n",
      "2015-01-01 00:57:31\n",
      "2015-01-01 00:58:52\n",
      "2015-01-01 01:00:37\n",
      "2015-01-01 01:00:57\n",
      "2015-01-01 01:01:25\n",
      "2015-01-01 01:02:50\n",
      "2015-01-01 01:03:24\n",
      "2015-01-01 01:03:44\n",
      "2015-01-01 01:03:46\n",
      "2015-01-01 01:04:29\n",
      "2015-01-01 01:05:20\n",
      "2015-01-01 01:05:34\n",
      "2015-01-01 01:05:56\n",
      "2015-01-01 01:07:16\n",
      "2015-01-01 01:08:12\n",
      "2015-01-01 01:08:35\n",
      "2015-01-01 01:09:56\n",
      "2015-01-01 01:10:02\n",
      "2015-01-01 01:10:40\n",
      "2015-01-01 01:13:37\n",
      "2015-01-01 01:13:42\n",
      "2015-01-01 01:13:45\n",
      "2015-01-01 01:14:19\n",
      "2015-01-01 01:16:57\n",
      "2015-01-01 01:17:06\n",
      "2015-01-01 01:17:18\n",
      "2015-01-01 01:17:36\n",
      "2015-01-01 01:17:58\n",
      "2015-01-01 01:18:00\n",
      "2015-01-01 01:18:20\n",
      "2015-01-01 01:18:49\n",
      "2015-01-01 01:18:50\n",
      "2015-01-01 01:19:47\n",
      "2015-01-01 01:20:19\n",
      "2015-01-01 01:20:36\n",
      "2015-01-01 01:20:44\n",
      "2015-01-01 01:21:43\n",
      "2015-01-01 01:23:34\n",
      "2015-01-01 01:23:41\n",
      "2015-01-01 01:24:28\n",
      "2015-01-01 01:25:52\n",
      "2015-01-01 01:25:58\n",
      "2015-01-01 01:28:25\n",
      "2015-01-01 01:28:57\n",
      "2015-01-01 01:29:57\n",
      "2015-01-01 01:30:19\n",
      "2015-01-01 01:30:53\n",
      "2015-01-01 01:31:24\n",
      "2015-01-01 01:32:29\n",
      "2015-01-01 01:33:12\n",
      "2015-01-01 01:33:14\n",
      "2015-01-01 01:34:16\n",
      "2015-01-01 01:34:24\n",
      "2015-01-01 01:35:50\n",
      "2015-01-01 01:36:38\n",
      "2015-01-01 01:39:01\n",
      "2015-01-01 01:39:19\n",
      "2015-01-01 01:39:46\n",
      "2015-01-01 01:39:56\n",
      "2015-01-01 01:40:14\n",
      "2015-01-01 01:43:44\n",
      "2015-01-01 01:44:41\n",
      "2015-01-01 01:44:58\n",
      "2015-01-01 01:45:45\n",
      "2015-01-01 01:45:53\n",
      "2015-01-01 01:47:20\n",
      "2015-01-01 01:47:30\n",
      "2015-01-01 01:48:15\n",
      "2015-01-01 01:49:11\n",
      "2015-01-01 01:49:44\n",
      "2015-01-01 01:50:18\n",
      "2015-01-01 01:50:31\n",
      "2015-01-01 01:53:24\n",
      "2015-01-01 01:54:14\n",
      "2015-01-01 01:54:29\n",
      "2015-01-01 01:54:41\n",
      "2015-01-01 01:55:13\n",
      "2015-01-01 01:55:58\n",
      "2015-01-01 01:56:08\n",
      "2015-01-01 01:57:21\n",
      "2015-01-01 02:02:00\n",
      "2015-01-01 02:02:51\n",
      "2015-01-01 02:03:04\n",
      "2015-01-01 02:03:11\n",
      "2015-01-01 02:05:03\n",
      "2015-01-01 02:08:27\n",
      "2015-01-01 02:09:11\n",
      "2015-01-01 02:10:50\n",
      "2015-01-01 02:11:54\n",
      "2015-01-01 02:11:56\n",
      "2015-01-01 02:13:22\n",
      "2015-01-01 02:13:24\n",
      "2015-01-01 02:14:03\n",
      "2015-01-01 02:14:21\n",
      "2015-01-01 02:17:41\n",
      "2015-01-01 02:18:22\n",
      "2015-01-01 02:18:49\n",
      "2015-01-01 02:19:22\n",
      "2015-01-01 02:22:06\n",
      "2015-01-01 02:23:53\n",
      "2015-01-01 02:24:08\n",
      "2015-01-01 02:27:04\n",
      "2015-01-01 02:28:20\n",
      "2015-01-01 02:29:40\n",
      "2015-01-01 02:30:22\n",
      "2015-01-01 02:31:19\n",
      "2015-01-01 02:31:21\n",
      "2015-01-01 02:33:30\n",
      "2015-01-01 02:33:31\n",
      "2015-01-01 02:33:34\n",
      "2015-01-01 02:35:07\n",
      "2015-01-01 02:35:22\n",
      "2015-01-01 02:35:43\n",
      "2015-01-01 02:36:23\n",
      "2015-01-01 02:37:42\n",
      "2015-01-01 02:39:55\n",
      "2015-01-01 02:40:11\n",
      "2015-01-01 02:40:40\n",
      "2015-01-01 02:43:44\n",
      "2015-01-01 02:46:11\n",
      "2015-01-01 02:46:42\n",
      "2015-01-01 02:46:46\n",
      "2015-01-01 02:48:03\n",
      "2015-01-01 02:48:07\n",
      "2015-01-01 02:51:20\n",
      "2015-01-01 02:53:11\n",
      "2015-01-01 02:53:22\n",
      "2015-01-01 02:53:52\n",
      "2015-01-01 02:56:21\n",
      "2015-01-01 02:57:06\n",
      "2015-01-01 02:57:54\n",
      "2015-01-01 02:59:45\n",
      "2015-01-01 02:59:48\n",
      "2015-01-01 03:00:18\n",
      "2015-01-01 03:02:42\n",
      "2015-01-01 03:03:38\n",
      "2015-01-01 03:05:09\n",
      "2015-01-01 03:06:21\n",
      "2015-01-01 03:06:59\n",
      "2015-01-01 03:07:14\n",
      "2015-01-01 03:08:31\n",
      "2015-01-01 03:08:53\n",
      "2015-01-01 03:11:22\n",
      "2015-01-01 03:11:33\n",
      "2015-01-01 03:12:18\n",
      "2015-01-01 03:13:31\n",
      "2015-01-01 03:15:14\n",
      "2015-01-01 03:15:56\n",
      "2015-01-01 03:16:03\n",
      "2015-01-01 03:19:30\n",
      "2015-01-01 03:20:01\n",
      "2015-01-01 03:21:50\n",
      "2015-01-01 03:22:11\n",
      "2015-01-01 03:25:22\n",
      "2015-01-01 03:25:28\n",
      "2015-01-01 03:25:45\n",
      "2015-01-01 03:25:46\n",
      "2015-01-01 03:28:10\n",
      "2015-01-01 03:29:46\n",
      "2015-01-01 03:32:39\n",
      "2015-01-01 03:33:51\n",
      "2015-01-01 03:34:01\n",
      "2015-01-01 03:36:01\n",
      "2015-01-01 03:36:02\n",
      "2015-01-01 03:36:33\n",
      "2015-01-01 03:38:41\n",
      "2015-01-01 03:39:34\n",
      "2015-01-01 03:40:22\n",
      "2015-01-01 03:41:14\n",
      "2015-01-01 03:41:46\n",
      "2015-01-01 03:42:22\n",
      "2015-01-01 03:42:47\n",
      "2015-01-01 03:43:03\n",
      "2015-01-01 03:43:21\n",
      "2015-01-01 03:43:49\n",
      "2015-01-01 03:44:54\n",
      "2015-01-01 03:45:33\n",
      "2015-01-01 03:47:11\n",
      "2015-01-01 03:47:43\n",
      "2015-01-01 03:48:51\n",
      "2015-01-01 03:50:10\n",
      "2015-01-01 03:50:16\n",
      "2015-01-01 03:52:43\n",
      "2015-01-01 03:53:59\n",
      "2015-01-01 03:54:07\n",
      "2015-01-01 03:54:53\n",
      "2015-01-01 03:56:50\n",
      "2015-01-01 03:56:54\n",
      "2015-01-01 03:57:31\n",
      "2015-01-01 03:58:00\n",
      "2015-01-01 03:58:17\n",
      "2015-01-01 03:59:04\n",
      "2015-01-01 04:02:22\n",
      "2015-01-01 04:02:25\n",
      "2015-01-01 04:03:00\n",
      "2015-01-01 04:05:19\n",
      "2015-01-01 04:06:11\n",
      "2015-01-01 04:09:20\n",
      "2015-01-01 04:12:12\n",
      "2015-01-01 04:12:16\n",
      "2015-01-01 04:13:52\n",
      "2015-01-01 04:15:11\n",
      "2015-01-01 04:15:31\n",
      "2015-01-01 04:16:44\n",
      "2015-01-01 04:17:50\n",
      "2015-01-01 04:19:39\n",
      "2015-01-01 04:23:11\n",
      "2015-01-01 04:23:44\n",
      "2015-01-01 04:26:23\n",
      "2015-01-01 04:27:12\n",
      "2015-01-01 04:27:14\n",
      "2015-01-01 04:29:38\n",
      "2015-01-01 04:30:23\n",
      "2015-01-01 04:30:40\n",
      "2015-01-01 04:31:29\n",
      "2015-01-01 04:33:02\n",
      "2015-01-01 04:33:04\n",
      "2015-01-01 04:33:12\n",
      "2015-01-01 04:35:15\n",
      "2015-01-01 04:35:17\n",
      "2015-01-01 04:36:19\n",
      "2015-01-01 04:37:40\n",
      "2015-01-01 04:41:12\n",
      "2015-01-01 04:41:46\n",
      "2015-01-01 04:42:14\n",
      "2015-01-01 04:43:02\n",
      "2015-01-01 04:43:17\n",
      "2015-01-01 04:44:52\n",
      "2015-01-01 04:44:57\n",
      "2015-01-01 04:45:52\n",
      "2015-01-01 04:47:32\n",
      "2015-01-01 04:50:06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-314309703201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \"&fromdate=1420070400&todate=1420156800&order=asc&sort=creation&site=stackoverflow\")\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mpullAllPages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-314309703201>\u001b[0m in \u001b[0;36mpullAllPages\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1420070400\u001b[0m \u001b[0;31m# 01-01-2015 at 00:00:00\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mend_time\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;36m1420156800\u001b[0m \u001b[0;31m# 01-02-2015 at 00:00:00\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         response = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100&page=\" + str(page) +\n\u001b[1;32m     15\u001b[0m                         \u001b[0;34m\"&fromdate=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"&todate=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1420088400\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def pullAllPages(response):\n",
    "    data = json.loads(response.text)\n",
    "    print_creation_dates_json(response)\n",
    "    page = 2\n",
    "    while(data['has_more'] == True):\n",
    "        start_time = 1420070400 # 01-01-2015 at 00:00:00\n",
    "        end_time   = 1420156800 # 01-02-2015 at 00:00:00\n",
    "        time.sleep(6)\n",
    "        response = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100&page=\" + str(page) +\n",
    "                        \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\n",
    "                        \"&order=asc&sort=creation&site=stackoverflow\")\n",
    "        page = page + 1\n",
    "        data = json.loads(response.text)\n",
    "        print_creation_dates_json(response)\n",
    "    \n",
    "response = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100&page=1\" +\n",
    "                        \"&fromdate=1420070400&todate=1420156800&order=asc&sort=creation&site=stackoverflow\")\n",
    "\n",
    "pullAllPages(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, we have downloaded the [data dump](http://cs-people.bu.edu/kzhao/teaching/stackoverflow-posts-2015.tar.gz) for Stack Overflow's posts in 2015. Note that this file is 10GB. If you don't have space on your computer, you can download it into `/scratch` on one of the machines in the undergrad lab or you can download it onto a USB. You may also want to work with a subset of this data at first, but your solution should be efficient enough to work with the whole dataset. For example, if you call `read()` on this file, you will get a `MemoryError`.\n",
    "\n",
    "Write a function to parse out the questions posted in 2015. These are posts with `PostTypeId=1`. Make a `pandas DataFrame` with 3 columns: `Id`, `CreationDate`, `OwnerUserId`, and the first tag in `Tags`. Save the `DataFrame` to a file named `question_dataframe.csv` using `to_csv()`. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "def fetchData():\n",
    "    #df = pd.DataFrame(columns=('Id', 'CreationDate', 'OwnerUserId', 'Tag'))\n",
    "    my_dict = {}\n",
    "    count = 0\n",
    "    for event, elem in etree.iterparse('stackoverflow-posts-2015.xml'):\n",
    "        #if(count > 2000):\n",
    "            #break\n",
    "        try:\n",
    "            if(elem.attrib['PostTypeId'] == '1'):\n",
    "                tag = elem.attrib['Tags']\n",
    "                tag = tag.split('>')[0][1:]\n",
    "                Id = elem.attrib['Id']\n",
    "                date = elem.attrib['CreationDate']\n",
    "                try:\n",
    "                    owner = elem.attrib['OwnerUserId']\n",
    "                except:\n",
    "                    owner = elem.attrib['OwnerDisplayName']\n",
    "            \n",
    "                list1 = [Id, date, owner,tag]\n",
    "                my_dict[count] = list1\n",
    "                list1 = []\n",
    "                count = count + 1\n",
    "            elem.clear()\n",
    "        except Exception,e: print str(e)\n",
    "    #print my_dict\n",
    "    df = pd.DataFrame(my_dict) \n",
    "    res = df.transpose()\n",
    "    res.columns = ['id', 'date', 'ownerId', 'tag']\n",
    "    res.to_csv('question_dataframe.csv')\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PostTypeId'\n",
      "         Unnamed: 0        id                     date     ownerId  \\\n",
      "0                 0  27727385  2015-01-01T00:00:58.253     3210431   \n",
      "1                 1  27727388  2015-01-01T00:01:43.673      868779   \n",
      "2                 2  27727391  2015-01-01T00:02:32.123     4372672   \n",
      "3                 3  27727393  2015-01-01T00:02:57.983     2482149   \n",
      "4                 4  27727394  2015-01-01T00:03:31.337     4263870   \n",
      "5                 5  27727396  2015-01-01T00:04:01.407     4409381   \n",
      "6                 6  27727406  2015-01-01T00:05:03.773      875317   \n",
      "7                 7  27727407  2015-01-01T00:05:27.167      821742   \n",
      "8                 8  27727408  2015-01-01T00:05:34.733     2595033   \n",
      "9                 9  27727409  2015-01-01T00:06:17.720     1815395   \n",
      "10               10  27727410  2015-01-01T00:06:41.067      541091   \n",
      "11               11  27727414  2015-01-01T00:07:28.747     1210038   \n",
      "12               12  27727418  2015-01-01T00:07:39.243     3674356   \n",
      "13               13  27727419  2015-01-01T00:07:46.460      347062   \n",
      "14               14  27727424  2015-01-01T00:09:28.247     1254618   \n",
      "15               15  27727427  2015-01-01T00:10:05.340     3412951   \n",
      "16               16  27727429  2015-01-01T00:10:29.963     1743377   \n",
      "17               17  27727433  2015-01-01T00:11:43.463      169992   \n",
      "18               18  27727434  2015-01-01T00:12:02.597     4056620   \n",
      "19               19  27727439  2015-01-01T00:12:55.513     2993567   \n",
      "20               20  27727442  2015-01-01T00:13:04.980     3085148   \n",
      "21               21  27727444  2015-01-01T00:13:32.583     2457761   \n",
      "22               22  27727446  2015-01-01T00:13:50.463     4371259   \n",
      "23               23  27727455  2015-01-01T00:15:40.250     3802790   \n",
      "24               24  27727459  2015-01-01T00:17:11.113     2227834   \n",
      "25               25  27727464  2015-01-01T00:18:50.297     4397115   \n",
      "26               26  27727468  2015-01-01T00:22:16.780     3494499   \n",
      "27               27  27727477  2015-01-01T00:23:14.747     3286192   \n",
      "28               28  27727481  2015-01-01T00:23:50.863     2999675   \n",
      "29               29  27727484  2015-01-01T00:24:28.103     4289580   \n",
      "...             ...       ...                      ...         ...   \n",
      "2530474     2530474  34552466  2015-12-31T23:42:01.600     5728911   \n",
      "2530475     2530475  34552470  2015-12-31T23:42:45.380     1763652   \n",
      "2530476     2530476  34552473  2015-12-31T23:44:33.663     4673488   \n",
      "2530477     2530477  34552480  2015-12-31T23:45:33.357     5735042   \n",
      "2530478     2530478  34552482  2015-12-31T23:45:48.310      831878   \n",
      "2530479     2530479  34552483  2015-12-31T23:45:48.627      510296   \n",
      "2530480     2530480  34552486  2015-12-31T23:46:08.790     3035850   \n",
      "2530481     2530481  34552487  2015-12-31T23:46:10.497     5735090   \n",
      "2530482     2530482  34552488  2015-12-31T23:46:12.693     5299726   \n",
      "2530483     2530483  34552492  2015-12-31T23:47:07.157     1108057   \n",
      "2530484     2530484  34552500  2015-12-31T23:48:17.143     5735067   \n",
      "2530485     2530485  34552501  2015-12-31T23:48:54.070     4838216   \n",
      "2530486     2530486  34552504  2015-12-31T23:49:14.810     2665223   \n",
      "2530487     2530487  34552506  2015-12-31T23:50:00.363       15441   \n",
      "2530488     2530488  34552508  2015-12-31T23:50:53.880      592888   \n",
      "2530489     2530489  34552510  2015-12-31T23:51:22.323     5009786   \n",
      "2530490     2530490  34552511  2015-12-31T23:51:27.817     5735089   \n",
      "2530491     2530491  34552513  2015-12-31T23:51:36.167      254046   \n",
      "2530492     2530492  34552515  2015-12-31T23:52:14.067      259602   \n",
      "2530493     2530493  34552519  2015-12-31T23:53:52.380     5618866   \n",
      "2530494     2530494  34552527  2015-12-31T23:54:25.837      973158   \n",
      "2530495     2530495  34552535  2015-12-31T23:56:12.397     2705042   \n",
      "2530496     2530496  34552536  2015-12-31T23:56:16.013     4991888   \n",
      "2530497     2530497  34552539  2015-12-31T23:57:03.073      441016   \n",
      "2530498     2530498  34552540  2015-12-31T23:57:08.807     5377088   \n",
      "2530499     2530499  34552543  2015-12-31T23:57:36.433     1040740   \n",
      "2530500     2530500  34552547  2015-12-31T23:58:06.150     3909898   \n",
      "2530501     2530501  34552549  2015-12-31T23:59:30.960     1870790   \n",
      "2530502     2530502  34552827  2015-12-31T06:02:40.277     2340452   \n",
      "2530503     2530503  34558008  2015-12-14T02:47:06.173  user207491   \n",
      "\n",
      "                    tag  \n",
      "0                   php  \n",
      "1            apache-pig  \n",
      "2                   ios  \n",
      "3            sql-server  \n",
      "4                   php  \n",
      "5               android  \n",
      "6                    c#  \n",
      "7                  java  \n",
      "8                    c#  \n",
      "9                apache  \n",
      "10                mysql  \n",
      "11           javascript  \n",
      "12                  php  \n",
      "13                 html  \n",
      "14              collada  \n",
      "15        ruby-on-rails  \n",
      "16           javascript  \n",
      "17                    c  \n",
      "18                  ios  \n",
      "19                 java  \n",
      "20           powershell  \n",
      "21                 java  \n",
      "22              node.js  \n",
      "23                  ios  \n",
      "24                    c  \n",
      "25           javascript  \n",
      "26                  ios  \n",
      "27              android  \n",
      "28                  c++  \n",
      "29                  php  \n",
      "...                 ...  \n",
      "2530474            java  \n",
      "2530475           mysql  \n",
      "2530476          python  \n",
      "2530477            java  \n",
      "2530478               k  \n",
      "2530479             dom  \n",
      "2530480            xmpp  \n",
      "2530481            ruby  \n",
      "2530482         windows  \n",
      "2530483             php  \n",
      "2530484             tcp  \n",
      "2530485           mysql  \n",
      "2530486       angularjs  \n",
      "2530487            java  \n",
      "2530488             api  \n",
      "2530489             osx  \n",
      "2530490             c++  \n",
      "2530491            ruby  \n",
      "2530492             uml  \n",
      "2530493               c  \n",
      "2530494            html  \n",
      "2530495          vb.net  \n",
      "2530496      javascript  \n",
      "2530497  actionscript-3  \n",
      "2530498           mysql  \n",
      "2530499            java  \n",
      "2530500             ios  \n",
      "2530501             c++  \n",
      "2530502           flask  \n",
      "2530503           mysql  \n",
      "\n",
      "[2530504 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fetchData()\n",
    "#question_df = pd.read_csv('question_dataframe_sample.csv')\n",
    "question_df = pd.read_csv('question_dataframe.csv')\n",
    "print question_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tackle our original problem. Write a function to measure the similarity of the top 1000 users with the most answer posts. Compare the users based on the types of questions they answer. We will categorize the questions by looking at the first tag in each question. You may choose to implement any one of the similarity/distance measures we discussed in class. Document your findings. **(30pts)**\n",
    "\n",
    "Note that answers are posts with `PostTypeId=2`. The ID of the question in answer posts is the `ParentId`.\n",
    "\n",
    "You may find the [sklearn.feature_extraction module](http://scikit-learn.org/stable/modules/feature_extraction.html) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ownerId\n",
      "1144035    7573\n",
      "548225     4303\n",
      "3732271    3392\n",
      "3297613    3101\n",
      "1491895    2954\n",
      "100297     2865\n",
      "3832970    2797\n",
      "2141635    2540\n",
      "114251     2414\n",
      "771848     2400\n",
      "6309       2359\n",
      "115145     2353\n",
      "1221571    2310\n",
      "157247     2310\n",
      "434551     2189\n",
      "335858     2094\n",
      "207421     2009\n",
      "22656      1959\n",
      "992484     1943\n",
      "19068      1911\n",
      "341994     1878\n",
      "795990     1852\n",
      "2422776    1841\n",
      "816620     1718\n",
      "2435473    1694\n",
      "462627     1685\n",
      "4039065    1663\n",
      "1501794    1588\n",
      "2025923    1557\n",
      "2877241    1557\n",
      "           ... \n",
      "4302471     260\n",
      "192373      260\n",
      "3695849     260\n",
      "2571212     259\n",
      "4363119     259\n",
      "1346234     259\n",
      "4628565     259\n",
      "960558      259\n",
      "1860929     259\n",
      "4687135     259\n",
      "696808      259\n",
      "76051       258\n",
      "283366      258\n",
      "294248      258\n",
      "5299236     258\n",
      "1464112     258\n",
      "363751      258\n",
      "4851590     258\n",
      "3970411     257\n",
      "354577      257\n",
      "2060725     257\n",
      "2405040     257\n",
      "3478010     257\n",
      "3282633     257\n",
      "1683264     257\n",
      "33258       257\n",
      "779513      256\n",
      "1672429     256\n",
      "33518       256\n",
      "4053652     256\n",
      "Name: parentId, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yili/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:35: FutureWarning: sort is deprecated, use sort_values(inplace=True) for for INPLACE sorting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ownerId\n",
       "1144035    7573\n",
       "548225     4303\n",
       "3732271    3392\n",
       "3297613    3101\n",
       "1491895    2954\n",
       "100297     2865\n",
       "3832970    2797\n",
       "2141635    2540\n",
       "114251     2414\n",
       "771848     2400\n",
       "6309       2359\n",
       "115145     2353\n",
       "1221571    2310\n",
       "157247     2310\n",
       "434551     2189\n",
       "335858     2094\n",
       "207421     2009\n",
       "22656      1959\n",
       "992484     1943\n",
       "19068      1911\n",
       "341994     1878\n",
       "795990     1852\n",
       "2422776    1841\n",
       "816620     1718\n",
       "2435473    1694\n",
       "462627     1685\n",
       "4039065    1663\n",
       "1501794    1588\n",
       "2025923    1557\n",
       "2877241    1557\n",
       "           ... \n",
       "4302471     260\n",
       "192373      260\n",
       "3695849     260\n",
       "2571212     259\n",
       "4363119     259\n",
       "1346234     259\n",
       "4628565     259\n",
       "960558      259\n",
       "1860929     259\n",
       "4687135     259\n",
       "696808      259\n",
       "76051       258\n",
       "283366      258\n",
       "294248      258\n",
       "5299236     258\n",
       "1464112     258\n",
       "363751      258\n",
       "4851590     258\n",
       "3970411     257\n",
       "354577      257\n",
       "2060725     257\n",
       "2405040     257\n",
       "3478010     257\n",
       "3282633     257\n",
       "1683264     257\n",
       "33258       257\n",
       "779513      256\n",
       "1672429     256\n",
       "33518       256\n",
       "4053652     256\n",
       "Name: parentId, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as etree\n",
    "def fetchAnswerPostsData():\n",
    "    count = 0;\n",
    "    tmp_dict = {}\n",
    "    for event, elem in etree.iterparse('stackoverflow-posts-2015.xml'):\n",
    "        #if(count > 1000):\n",
    "            #break\n",
    "        try:\n",
    "            if(elem.attrib['PostTypeId'] == '2'):\n",
    "                try:\n",
    "                    owner = elem.attrib['OwnerUserId']\n",
    "                except:\n",
    "                    owner = elem.attrib['OwnerDisplayName']\n",
    "                Id = elem.attrib['Id']\n",
    "                parent = elem.attrib['ParentId']\n",
    "                buff = [Id, owner, parent]\n",
    "                tmp_dict[count] = buff\n",
    "                count = count + 1\n",
    "            elem.clear()\n",
    "        except Exception,e: print str(e)\n",
    "    df = pd.DataFrame(tmp_dict)  \n",
    "    ans = df.transpose()\n",
    "    ans.columns = ['id','ownerId','parentId']\n",
    "    ans.to_csv('answer_dataframe.csv', encoding='utf-8')\n",
    "    print 'end'\n",
    "    return\n",
    " \n",
    "def top_Users():\n",
    "    answer_df = pd.read_csv('answer_dataframe.csv')\n",
    "    answer_df = answer_df.groupby(['ownerId']).count()\n",
    "    #make a new copy\n",
    "    res = answer_df['parentId'].copy()\n",
    "    #sort by descending order\n",
    "    res.sort(ascending = False)\n",
    "    #fetch the top 1000 users on the list\n",
    "    res = res.head(1000)\n",
    "    print res\n",
    "    return res\n",
    "    \n",
    "def users_questions_tags(list):\n",
    "    \n",
    "#fetchAnswerPostsData()\n",
    "top_Users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the top 100 most similar users. See [Lecture 3](https://github.com/datascience16/lectures/blob/master/Lecture3/Distance-Functions.ipynb) for examples. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#fetchAnswerPostsData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create some time series from the data. Look at the top 100 users with the most question posts. For each user, your time series will be the `CreationDate` of the questions posted by that user. You may want to make multiple time series for each user based on the first tag of the questions. Compare the time series using one of the methods discussed in class. Document your findings. **(30 pts)**\n",
    "\n",
    "You may find the [pandas.DataFrame.resample module](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2 most similar and the 2 most different time series. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
